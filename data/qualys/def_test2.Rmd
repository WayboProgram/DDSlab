---
title: "Qualys - Naive Bayes"
author: "Marc Gracia"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tm)
library(caret)
library(ggplot2)
library(tidyr)
library(reshape2)
library(knitr)
library(kableExtra)

```


Este ejercicio implica trabajar con un código preexistente que se encarga de predecir si una vulnerabilidad es crítica o no a partir del diagnóstico de dicha vulnerabilidad. Los datos utilizados para esta tarea son extraídos de un documento XML de Qualys que contiene información sobre diversas vulnerabilidades. A partir de este documento, se extraen los campos relevantes (ID, nivel de criticidad y diagnóstico) que son utilizados para entrenar el modelo de Machine Learning Naive Bayes.


<h3>Objetivo del ejercicio</h3>

El objetivo principal de este ejercicio es mejorar el rating del **accuracy** del modelo, el cual es de <font color="red">0,62</font> en su estado actual.


Para lograr este objetivo, se han realizado varias modificaciones en el código original con la finalidad de mejorar la precisión de las predicciones. Estas modificaciones incluyen:


- Intentar cambiar el modelo de Machine Learning utilizado. Se probó con los modelos Random Forests y Gradient Boosting Machines, pero se requieren otros parámetros para su correcto funcionamiento.

- Modificar el parámetro de la función <font color="blue">"removeSparseTerms()"</font> de <font color="red">0,85</font> a <font color="red">0,75</font>. Este parámetro se encarga de recolectar únicamente el **15%** de las palabras más frecuentes, pero no se observó un cambio significativo en el **accuracy** al modificar este valor.

- Realizar una limpieza de palabras en el código para eliminar palabras que se repiten con frecuencia, incluyendo versiones en singular y plural. Sin embargo, esta modificación no tuvo un efecto positivo en el rating del **accuracy**, ya que disminuyó a <font color="red">0,61</font>.


```{text example}
# Remove plural words
course_corpus5 <- tm_map(course_corpus4, content_transformer(function(x) gsub("\\b(\\w+)(s)\\b", "\\1", x)))

```

A pesar de estas ideas que no han funcionado, seguí trabajando para encontrar una solución que permita mejorar el rating del "accuracy" del modelo.

<h3>Idea para conseguir el objetivo</h3>

En este apartado, se presenta una idea para mejorar el código. Al observar que se utilizaban 2000 muestras para cada uno de los dos bloques (vulnerabilidades críticas y no críticas), se decidió realizar diferentes combinaciones de muestras que siempre sumasen un total de 4000. El objetivo de estas pruebas era determinar si esta variación en el tamaño de la muestra tendría un impacto positivo en la precisión del modelo, es decir, en su **accuracy**.

A continuación, se muestra una matriz con los resultados obtenidos tras las pruebas realizadas.


```{r read_raw, include=FALSE}

#*******************************************************************
#                         Código
#*******************************************************************

# Read XML document
raw.file = "latest.qkdb.xml.zip"
doc <- xml2::read_xml(raw.file)

# Extract QID, SEVERITY_LEVEL and DIAGNOSIS
kdb_txt <- rvest::html_text(rvest::html_elements(doc, xpath="//VULN[DIAGNOSIS]/*[self::QID or self::SEVERITY_LEVEL or self::DIAGNOSIS]"))
kdb_txt <- matrix(kdb_txt, nrow = length(kdb_txt)/3, ncol = 3, byrow = TRUE)
kdb_txt <- as.data.frame.matrix(kdb_txt)
names(kdb_txt) <- c("qid", "severity", "diagnosis")
```


```{r tidy_data, include=FALSE}
# Tidy data frame
kdb_txt$qid <- as.integer(kdb_txt$qid)
kdb_txt$severity <- as.integer(kdb_txt$severity)
kdb_txt$diagnosis <- textclean::replace_html(kdb_txt$diagnosis)
kdb_txt$critical <- ifelse(test = kdb_txt$severity < 5, yes = "NO", no = "YES")
kdb_txt$criticalb <- kdb_txt$severity == 5
```

```{r test_analysis, include=FALSE}
# Text analysis
## Stopwords
freq_word <- sort(table(unlist(strsplit(kdb_txt$diagnosis, " "))), decreasing = TRUE)
kdb_words <- names(freq_word)[(which(!(names(freq_word) %in% stopwords::stopwords())))]
## Characters
freq_char <- sort(table(unlist(strsplit(kdb_txt$diagnosis, ""))), decreasing = TRUE)

kdb_txt$descr <- textclean::replace_symbol(kdb_txt$diagnosis)
freq_char2 <- sort(table(unlist(strsplit(kdb_txt$descr, ""))), decreasing = TRUE)
freq_char2
```

```{r function, include=FALSE}
# Función para entrenar el modelo y devolver la precisión
train_model <- function(critical_size, other_size) {
  
  # Sample data
  kdb_critical <- kdb_txt %>% filter(critical == "YES") %>% sample_n(critical_size)
  kdb_other <- kdb_txt %>% filter(critical == "NO") %>% sample_n(other_size)
  
  kdb_ml <- bind_rows(kdb_critical, kdb_other) %>% 
    sample_n(critical_size + other_size) %>% 
    select(descr, critical)
  
  # Prepare data for training
  course_raw <- kdb_ml$descr
  course_corpus <- VCorpus(VectorSource(course_raw))
  
  # Clean text
  course_corpus2 <- tm_map(course_corpus, content_transformer(tolower))
  course_corpus3 <- tm_map(course_corpus2, removePunctuation)
  course_corpus4 <- tm_map(course_corpus3, removeWords, stopwords())
  
  # Generate TF-IDF matrix
  course_dtm <- DocumentTermMatrix(course_corpus4)
  dense_course_dtm <- removeSparseTerms(course_dtm, .85)
  conv_counts <- function(x) {
    x <- ifelse(x > 0, 1, 0)
    x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
  }
  class_dtm <- apply(dense_course_dtm, MARGIN = 2, conv_counts)
  
  # Split data into training and testing sets
  train_set <- createDataPartition(y = kdb_ml$critical, p = .7, list = FALSE)
  train_dtm <- class_dtm[train_set, ]
  test_dtm <- class_dtm[-train_set, ]
  train_classes <- kdb_ml$critical[train_set]
  test_classes <- kdb_ml$critical[-train_set]
  
  # Train the model using Naive Bayes
  course_model <- train(data.frame(train_dtm), train_classes, method = "nb")
  
  # Predict for the test data
  course_predictions <- predict(course_model, test_dtm)
  
  # Return the confusion matrix
  confusionMatrix(table(course_predictions, test_classes))$overall["Accuracy"]
}
```

```{r define_samples, include=FALSE}
# Set the different combinations of sizes to try
sample_sizes <- list(critical_size = c(50, 500, 1000, 2000, 3000, 3500, 3950),
                     other_size = c(3950, 3500, 3000, 2000, 1000, 500, 50))

# Filter the sample sizes to only include combinations that sum to 4000
filtered_sizes <- expand.grid(sample_sizes) %>%
  filter(rowSums(.) == 4000)
```

```{r train_model, include=FALSE}
# Train the model for each combination and store the results
results <- filtered_sizes %>%
  rowwise() %>%
  mutate(accuracy = train_model(critical_size, other_size))
```

```{r print_table, include=FALSE}
# Convert results_table to a matrix
results_matrix <- acast(results, other_size ~ critical_size, value.var = "accuracy")

row.names = c("50", "500", "1000", "2000", "3000", "3500", "3950") 
col.names = c("50", "500", "1000", "2000", "3000", "3500", "3950")

# Eliminar los nombres de fila y columna
rownames(results_matrix) <- row.names
colnames(results_matrix) <- col.names

results_matrix2 <- as.matrix(results_matrix)

print(results_matrix2)
```

```{r show_matrix, include=FALSE}
table <- kable(results_matrix2, align = "c", 
      caption = "Tabla de resultados",
      missing = "NA")
```


```{r show_table, echo=FALSE}
table
```



En la tabla se pueden observar los diferentes resultados obtenidos dependiendo de las combinaciones realizadas. 

Con el fin de visualizar mejor la distribución de los resultados y obtener conclusiones relevantes, se ha representado esta información en forma de gráfica.

```{r plot_results, include=FALSE}
# Plot the results
show_plot <- ggplot(results, aes(x = critical_size, y = other_size)) +
  geom_point(aes(size = accuracy), shape = 21, fill = "steelblue") +
  scale_size(range = c(3, 15)) +
  labs(title = "Accuracy's de las diferentes combinaciones",
       x = "Nº de vulnerabilidades críticas",
       y = "Nº de vulnerabilidades no críticas") +
  theme(legend.position = "none") 

```

```{r show_plot, echo=FALSE}

show_plot

```


En la figura de arriba, se observa una distribución casi simétrica en toda la diagonal. Esta distribución indica que cuando la balanza de muestras se inclina hacia un lado, la bola correspondiente es mayor. Una bola más grande representa un mejor porcentaje de precisión del modelo, lo que se conoce como **accuracy**.

<h4>**Conclusión**</h4>

Mi interpretación es que cuando se utiliza una muestra más grande de vulnerabilidades críticas o no críticas para entrenar el modelo, éste se especializa en detectar uno de los dos tipos. Por ejemplo, si el modelo se entrena con una muestra mucho mayor de vulnerabilidades críticas que no críticas, se centrará en detectar las vulnerabilidades críticas. En consecuencia, si el modelo no encuentra similitudes en el **diagnóstico** de una vulnerabilidad con una crítica, la descarta.

Es importante recordar que el modelo original utilizaba el mismo número de muestras para ambos tipos, lo que limitaba su capacidad de detectar uno u otro tipo de vulnerabilidad. Al proporcionar una muestra mayor de uno de los dos tipos, el modelo mejora significativamente y puede acercarse incluso a un **modelo perfecto**.